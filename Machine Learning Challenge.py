# -*- coding: utf-8 -*-
"""Kovacevic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jhj9uGa5mkiSa8dBjrDrX3pzYM-3ak-Q

# <b><u> MACHINE LEARNING CHALLENGE </u></b>

# Packages loading
"""

! pip install optuna
! pip install xgboost

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy import stats
from imblearn.over_sampling import RandomOverSampler
import xgboost as xgb
import optuna
from sklearn.model_selection import train_test_split
from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.dummy import DummyClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import GridSearchCV
from sklearn import metrics

"""### <b> LOADING DATA</b>

"""

data = pd.read_csv('Newdata-2.csv')

"""### <b> Exploratory Data Analysis (EDA)</b>

"""

# General information about the data set
print ("DATA INFO")

print(data.info())

print ("----------")

print("SIZE")
print (data.size)
print ('-----------')

print("SHAPE")
print (data.shape)

print ('----------')
print("DESCRIPTION")
data.describe().transpose()

# Inspection of the unique values for each feature
for column in data.columns:
    unique_values = data[column].unique()
    print("Unique values for column -> {}:".format(column))
    print(unique_values)
    print()

"""### <b> DATA PREPROCESSING</b>"""

# Performing label encoding on categorical variable (Customer_Type) in order to convert them into numerical features
le = LabelEncoder()

data['Customer_Type'] = le.fit_transform(data['Customer_Type'])

# Convertion of the only boolean variable into numerical with the same way
data['Weekday'] = le.fit_transform(data['Weekday'])

# Checking the data after the procedure of transformation
data.info()
# Both Customer Type and Weekdays are in the preferable type

"""*For better understanding of the data we proceed to create a boxplot and a histogram for each variable.*"""

# Figure size and layout
fig, axes = plt.subplots(nrows=22, ncols=2, figsize=(12, 3 * len(data.columns)+4))
fig.tight_layout(pad=2.0)


for index, column in enumerate(data):
    # Plot boxplot
    sns.boxplot(data=data[column], ax=axes[index, 0])
    axes[index, 0].set_title(f"Boxplot of {column}")
    axes[index, 0].set_xlabel("")

    # Plot distribution
    sns.histplot(data=data, x=column, kde=True, ax=axes[index, 1])
    axes[index, 1].set_title(f"Distribution of {column}")
    axes[index, 1].set_xlabel("")

# Save the figure as a JPEG image
plt.savefig('Boxplot_Histogram.jpg')
# Interpret of graphs
plt.show()

"""#### <b> Correlation Matrix intrepreted using Heatmap to detect the correlation between the features and the target variable</b>"""

# Separation the target column
target_col = 'Transaction'


# Calculate the correlation matrix for thee encoded_data
correlation_matrix = data.corr(method='pearson')  # we choose Pearson, as it is the default parameter of the corr function

# Create a heatmap of the correlation matrix
plt.figure(figsize=(13, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.show()

"""From above histograms and boxplots it is obvious that the Google_Analytics_PV has a lot of outliers.
Proceeding to implemention of Z-score to reduce this problem, as the future importance at the last stage of the code showed that it is the most important feature
"""

z_scores = np.abs((data['GoogleAnalytics_PV'] - data['GoogleAnalytics_PV'].mean()) / data['GoogleAnalytics_PV'].std())
data.loc[z_scores > 3, 'GoogleAnalytics_PV'] = 2

"""Over sampling method for the target variable (manual way)"""

X_ones= data[data['Transaction'] == 1]
X_zeroes = data[data['Transaction'] == 0]
X_zeroes = X_zeroes.sample(n=1908, random_state=42)
X = pd.concat([X_ones, X_zeroes], axis=0)

# Splitting the data into Target and Features
y = X['Transaction']

X = X.drop('Transaction',axis=1)

"""*Checking the distribution of the target variable*

"""

target_counts = y.value_counts()


# Plot the class distribution
plt.figure(figsize=(8, 6))
target_counts.plot(kind='bar')
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Class Distribution')
plt.show()

# Compute class proportions
class_proportions = target_counts / target_counts.sum()
print("\nClass proportions:")
print(class_proportions)

"""*   Split the data into train and test sets
*   The test_size parameter is set to 0.2, indicating that 20 % of the data will go for the test set (80% for train set)

*   The random_state parameter is set to 42 to ensure reproducibility of the splits
*   The stratify parameter is set to y, which ensures that the class distribution in the original dataset is preserved in all three sets (training, validation, and test)

"""

# Splitting data to Train set and Test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Splitting Train set to Train set and Validation set
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)

"""## BASE LINE MODEL XGBOOST"""

# Create an instance of the XGBoost Classifier model
model_xgb = xgb.XGBClassifier( )

# Fit the model on the training data
model_xgb.fit(X_train, y_train)

# Predict on the validation set
y_val_pred_XGB_Base = model_xgb.predict(X_val)

# Calculating the performance scores of the model on the validation set
precision_xgb_base = precision_score(y_val, y_val_pred_XGB_Base)
recall_xgb_base = recall_score(y_val, y_val_pred_XGB_Base)
f1_xgb_base = f1_score(y_val, y_val_pred_XGB_Base)

print("Precision: {:.2f}%".format(precision_xgb_base*100))
print("Recall: {:.2f}%".format(recall_xgb_base*100))
print("F1 Score: {:.2f}%".format(f1_xgb_base*100))

"""Interpreting the confusion matrix for the XGBoost baseline model"""

confusion_matrix = metrics.confusion_matrix(y_val, y_val_pred_XGB_Base)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.title('Confusion Matrix - XGBoost Base')
# Save the figure as a JPEG image
plt.savefig('Con_matrix_XGB_base.jpg')
plt.show()

"""## XGBOOST HYPER-PARAMETER TUNNING"""

# Definition of the objective function that will be used to be minimized by Optuna
def objective(trial):
    # Definition of the parameters that will be checked and their range
    params ={
        'n_estimators': trial.suggest_int('n_estimators', 20, 200),
        'max_depth': trial.suggest_int('max_depth', 3, 15),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 2),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10),
    }

    # Create the XGBoost classifier with the specified parameters
    model_xgb_tunned = xgb.XGBClassifier(**params)

    # Train the model
    model_xgb_tunned.fit(X_train, y_train)

    # Predict on the validation set
    y_pred = model_xgb_tunned.predict(X_val)

    # Calculate F1 score
    f1 = f1_score(y_val, y_pred)

    return f1

# Create an Optuna study and optimize the objective function created above
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

# Print the best hyperparameters and the corresponding F1 score
best_params = study.best_params
best_f1_score = study.best_value

print("Best Hyperparameters:")
print(best_params)
print("Best F1 Score:", best_f1_score)

"""we fit the model again but with concat now
Using the optimal parameters found by optuna to fit the model again but on the Train + Validation set in order to imporve generalization
"""

final_model_xgb= xgb.XGBClassifier(**best_params)

# Combine the training and validation sets
X_train_val = pd.concat([X_train, X_val])
y_train_val = pd.concat([y_train, y_val])

# Train the model on the combined training + validation set
final_model_xgb.fit(X_train_val, y_train_val)

# Predict on the test set
y_pred_test_XGB_TUNNED = final_model_xgb.predict(X_test)

# Calculate performance metrics on the test set
precision_test_xgb_tunned_VT = precision_score(y_test, y_pred_test_XGB_TUNNED)
recall_test_xgb_tunned_VT = recall_score(y_test, y_pred_test_XGB_TUNNED)
f1_score_test_xgb_tunned_VT = f1_score(y_test, y_pred_test_XGB_TUNNED)

# Print the evaluation metrics
print("Test Set Metrics:")
print("Precision: {:.2f}%".format(precision_test_xgb_tunned_VT * 100))
print("Recall: {:.2f}%".format(recall_test_xgb_tunned_VT * 100))
print("F1 Score: {:.2f}%".format(f1_score_test_xgb_tunned_VT * 100))

"""Interpreting the confusion matrix for the XGBoost final model( after tunning and Train + Validation set)**bold text**"""

confusion_matrix = metrics.confusion_matrix(y_test, y_pred_test_XGB_TUNNED)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.title('Confusion Matrix - XGBoost Tunned')
# Save the figure as a JPEG image
plt.savefig('Con_matrix_XGB_Tunned.jpg')
plt.show()

"""Last step of the XGBoost model is to calculate the performance scores based on 5 - fold validation to understand it's behavior on generalization"""

# Define the scoring metrics that will be used in the 5 - fold validaiton
scoring = {
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1_score': make_scorer(f1_score)
}


# Calculate cross-validation scores for each metric
precision_scores_xgb_5fold = cross_val_score(final_model_xgb, X_test, y_test, cv=5, scoring='precision')
recall_scores_xgb_5fold = cross_val_score(final_model_xgb, X_test, y_test, cv=5, scoring='recall')
f1_scores_xgb_5fold = cross_val_score(final_model_xgb, X_test, y_test, cv=5, scoring='f1')

# Calculate average scores
avg_precision_xgb_5fold = precision_scores_xgb_5fold.mean()
avg_recall_xgb_5fold = recall_scores_xgb_5fold.mean()
avg_f1_xgb_5fold = f1_scores_xgb_5fold.mean()

print("Average Precision:", avg_precision_xgb_5fold)
print("Average Recall:", avg_recall_xgb_5fold)
print("Average F1 Score:", avg_f1_xgb_5fold)

"""#### ** DECISION TREES BASELINE MODEL **


"""

# Create an instance of the Decission Tree Classifier model
DT_baseline_model = DecisionTreeClassifier(random_state=42)

#Fit the model to the training data
DT_baseline_model.fit(X_train, y_train)

#We predict on validation set
y_pred_base_DT = DT_baseline_model.predict(X_val)

#Evaluate the baseline model's performance
precision_DT_base = precision_score(y_val, y_pred_base_DT)
recall_DT_base = recall_score(y_val, y_pred_base_DT)
f1_DT_base = f1_score(y_val, y_pred_base_DT)

print("Precision: {:.2f}%".format(precision_DT_base*100))
print("Recall: {:.2f}%".format(recall_DT_base*100))
print("F1 Score: {:.2f}%".format(f1_DT_base*100))

"""Interpreting the confusion matrix for the Decission Tree base model"""

#confussion matrix dec trees

confusion_matrix = metrics.confusion_matrix(y_val, y_pred_base_DT)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.title('Confusion Matrix - Decission Tree Base')
# Save the figure as a JPEG image
plt.savefig('Con_matrix_DT_base.jpg')
plt.show()

"""####**LOGISTIC REGRESSION BASELINE MODEL**

"""

# Create an instance of the Logistic Regression model
logreg_model = LogisticRegression(random_state=42  , solver='liblinear')

# Fit the model on the training data
logreg_model.fit(X_train, y_train)

# Predict on validation set
y_pred_base_LR = logreg_model.predict(X_val)

# Evaluate the baseline model's performance
precision_LR_base = precision_score(y_val, y_pred_base_LR)
recall_LR_base = recall_score(y_val, y_pred_base_LR)
f1_LR_base = f1_score(y_val, y_pred_base_LR)

print("Precision: {:.2f}%".format(precision_LR_base*100))
print("Recall: {:.2f}%".format(recall_LR_base*100))
print("F1 Score: {:.2f}%".format(f1_LR_base*100))

"""Interpreting the confusion matrix for the Logistic Regression base model"""

confusion_matrix = metrics.confusion_matrix(y_val, y_pred_base_LR)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.title('Confusion Matrix - Logistic Regression Base')
# Save the figure as a JPEG image
plt.savefig('Con_matrix_LG_base.jpg')
plt.show()

"""## LOGISTIC REGRESSION BOOSTING"""

# Define the hyperparameter grid
param_grid = {
    'C': [0.01, 0.1, 1.0, 5.0],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga'],
    'max_iter': [40, 70, 100]}


# Perform GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(logreg_model, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Retrieve the best hyperparameters and corresponding score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Hyperparameters:", best_params)
print("Best Score:", best_score)

# Train the model with the best hyperparameters on the entire training set
model_lr_tunned = LogisticRegression(**best_params)
model_lr_tunned.fit(X_val, y_val)

# Evaluate the model on the test set
y_pred_LG_BOOST = model_lr_tunned.predict(X_test)


# Define the scoring metrics
scoring = {
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1_score': make_scorer(f1_score)
}

# Calculate cross-validation scores for each metric
precision_scores_LR_tunned = cross_val_score(model_lr_tunned, X_test, y_test, cv=5, scoring='precision')
recall_scores_LR_tunned = cross_val_score(model_lr_tunned, X_test, y_test, cv=5, scoring='recall')
f1_scores_LR_tunned = cross_val_score(model_lr_tunned, X_test, y_test, cv=5, scoring='f1')

# Calculate average scores
avg_precision_LR_tunned = precision_scores_LR_tunned.mean()
avg_recall_LR_tunned = recall_scores_LR_tunned.mean()
avg_f1_LR_tunned = f1_scores_LR_tunned.mean()

# Print the evaluation metrics
print("Test Set Metrics:")
print("Precision: {:.2f}%".format(avg_precision_LR_tunned * 100))
print("Recall: {:.2f}%".format(avg_recall_LR_tunned * 100))
print("F1 Score: {:.2f}%".format(avg_f1_LR_tunned * 100))

"""Interpreting the confusion matrix for the Logistic Regression final model( after tunning and Train + Validation set)**bold text**"""

confusion_matrix = metrics.confusion_matrix(y_test, y_pred_LG_BOOST)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.title('Confusion Matrix - Logistic Regression Tunned')
# Save the figure as a JPEG image
plt.savefig('Con_matrix_LG_Tunned.jpg')
plt.show()

"""Implementing 5- Validation and checking the performance"""

# Train the model on the combined training and validation sets
model_lr_tunned.fit(X_train_val, y_train_val)

# Predict on the test set
y_pred_test_lg_concat = model_lr_tunned.predict(X_test)

# Calculate evaluation metrics on the test set
precision_test_LG_5fold = precision_score(y_test, y_pred_test_lg_concat)
recall_test_LG_5fold = recall_score(y_test, y_pred_test_lg_concat)
f1_score_test_LG_5fold = f1_score(y_test, y_pred_test_lg_concat)

# Print the evaluation metrics
print("Test Set Metrics:")
print("Precision: {:.2f}%".format(precision_test_LG_5fold * 100))
print("Recall: {:.2f}%".format(recall_test_LG_5fold * 100))
print("F1 Score: {:.2f}%".format(f1_score_test_LG_5fold * 100))

"""__REPORTS A SUMMARY OF THE MODEL'S PERFORMANCES FOR ITS MODEL__"""

print ('BASELINE MODELS')

print ('BASELINE DECISION TREE BOOST REPORT')
print()
report_base_DT = classification_report(y_val, y_pred_base_DT)
print (report_base_DT)

print()
print ('BASELINE XGD BOOST REPORT')
report_base_xgd = classification_report(y_val, y_val_pred_XGB_Base)
print (report_base_xgd)

print()
print('BASELINE LOGISTIC REGRESSION REPORT')
report_base_LR = classification_report(y_val , y_pred_base_LR)
print (report_base_LR)

print()
print('LOGISTIC REGRESSION BOOST REPORT')
report_base_LR = classification_report(y_test , y_pred_LG_BOOST)
print (report_base_LR)



print ()
print ('TUNING XG BOOST REPORT')
report_tuning_xgb = classification_report(y_test , y_pred_test_XGB_TUNNED)
print(report_tuning_xgb)

"""Interpreration of the future importance based on the XGBoost Classifier after Tunning"""

# Get the feature importances
importances = final_model_xgb.feature_importances_

# Create a DataFrame to store feature names and importances
feature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': importances})

# Sort the features by importance in descending order
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(15, 10))
plt.bar(feature_importances['Feature'], feature_importances['Importance'])
plt.xlabel('Features')
plt.ylabel('Feature_Importances')
plt.xticks(rotation=90)
plt.title('Feature_Importances')
plt.tight_layout()
plt.show()

all_classifiers = ['Logistic Regression Base', 'Decision Tree Base', 'XG Boosting Base' ]
all_precision_score = [precision_DT_base, precision_LR_base, precision_xgb_base]
all_recall_score = [recall_LR_base, recall_DT_base, recall_xgb_base]
all_f1_score = [f1_LR_base, f1_DT_base, f1_xgb_base]

all_tunned_precision_score = [precision_test_LG_5fold ,precision_test_xgb_tunned_VT]
all_tunned_recall_score = [recall_test_LG_5fold,recall_test_xgb_tunned_VT]
all_tunned_f1_score = [f1_score_test_LG_5fold,f1_score_test_xgb_tunned_VT]
all_avg_precision_score = [avg_precision_LR_tunned, avg_precision_xgb_5fold]
all_avg_recall_score = [avg_recall_LR_tunned, avg_recall_xgb_5fold]
all_avg_f1_score = [avg_f1_LR_tunned, avg_f1_xgb_5fold]


# Dataframe with three columns
df_three_columns = pd.DataFrame({'Classifier': ['Logistic Regression Base', 'Decision Tree Base', 'XG Boosting Base' ],
                                 'Precision': all_precision_score,
                                 'Recall': all_recall_score,
                                 'F1 Score': all_f1_score})

# Dataframe with two columns
df_two_columns = pd.DataFrame({'Classifier': ['Logistic Regression', 'XG Boosting'],
                               'Avg Precision (5-fold CV)': all_avg_precision_score,
                               'Avg Recall (5-fold CV)': all_avg_recall_score,
                               'Avg F1 Score (5-fold CV)': all_avg_f1_score})

# Dataframe for tuned models
df_tuned = pd.DataFrame({'Classifier': ['Logistic Regression', 'XG Boosting'],
                         'Tuned Precision': all_tunned_precision_score,
                         'Tuned Recall': all_tunned_recall_score,
                         'Tuned F1 Score': all_tunned_f1_score})

# Save dataframes as images
df_three_columns.to_csv('df_three_columns.csv', index=False)
df_two_columns.to_csv('df_two_columns.csv', index=False)
df_tuned.to_csv('df_tuned.csv', index=False)

# Plot and save the images
fig, ax = plt.subplots(figsize=(12, 8))
ax.axis('off')  # Hide axis

# Plot dataframe with three columns
ax.table(cellText=df_three_columns.values,
         colLabels=df_three_columns.columns,
         cellLoc='center',
         loc='upper left')

plt.savefig('df_three_columns.png')

# Plot dataframe with two columns
fig, ax = plt.subplots(figsize=(12, 8))
ax.axis('off')  # Hide axis

ax.table(cellText=df_two_columns.values,
         colLabels=df_two_columns.columns,
         cellLoc='center',
         loc='upper left')

plt.savefig('df_two_columns.png')

# Plot dataframe for tuned models
fig, ax = plt.subplots(figsize=(12, 8))
ax.axis('off')  # Hide axis

ax.table(cellText=df_tuned.values,
         colLabels=df_tuned.columns,
         cellLoc='center',
         loc='upper left')

plt.savefig('df_tuned.png')

"""Create a data frame of the ground trouth values and the predictions.
Save it as a csv *file*
"""

y_pred_test_XGB_TUNNED= pd.DataFrame(y_pred_test_XGB_TUNNED)
y_pred_test_XGB_TUNNED.columns= ['y_predict']
y_test = pd.DataFrame(y_test)
y_test.columns = ['y_hat']
predictions = pd.concat([y_test,y_pred_test_XGB_TUNNED],axis = 1)

y_test = y_test.reset_index(drop=True)
y_pred_test_XGB_TUNNED = y_pred_test_XGB_TUNNED.reset_index(drop=True)

predictions = pd.concat([y_test, y_pred_test_XGB_TUNNED], axis=1)

predictions.to_csv('Predictions.csv', index=False)